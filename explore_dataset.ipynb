{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/e-gmd-v1.0.0'\n",
    "slim_metadata_df = pd.read_csv('dataset/e-gmd-v1.0.0-slim.csv') # See `create_slim_metadata.py` for details.\n",
    "note_occurrences_slim_df = pd.read_csv('dataset/note_occurrences_slim.csv') # See `create_label_mappings.py` for details.\n",
    "labels_df = pd.read_csv('dataset/label_mapping.csv') # See `create_label_mappings.py` for details.\n",
    "chopped_df = pd.read_csv('dataset/chopped.csv') # See `chop_dataset.py` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note occurrences in slim dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_occurrences_slim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Note Occurrences')\n",
    "_ = plt.bar(note_occurrences_slim_df['name'], note_occurrences_slim_df['occurrences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label mappings\n",
    "\n",
    "The top-5 most frequencly occurring drum instrument types are used for training.\n",
    "\n",
    "The label mappings contain a row for each training drum instrument, with the following columns:\n",
    "- `id`: Used for one-hot encoding during training. Corresponds to the instrument's occurrence frequency rank in the slim dataset, with the smallest value corresponding to the most common.\n",
    "- `note`: The MIDI note of the drum instrument.\n",
    "- `name`: The human-readable name of the drum instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_name = lambda label: labels_df.iloc[label]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chopped dataset\n",
    "\n",
    "The \"chopped\" dataset is the final, processed dataset used for training.\n",
    "\n",
    "It consists of a row per \"drum hit\", which is composed of one or more simultaneously sounding drum instruments, and it has the following columns:\n",
    "- `file_path`: The path to the audio file in the E-GMD dataset.\n",
    "- `begin_frame`: The frame (sample index) of the beginning of the hit.\n",
    "- `num_frames`: The length, in frames, of the hit.\n",
    "- `label`: A drum instrument label, corresponding to the `id` column in the `dataset/label_mapping.csv` file generated by the `create_label_mapping.py` script.\n",
    "- `slim_id`: The session ID (index in `e-gmd-v1.0.0-slim.csv`) in which this hit was found, for access to any other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chopped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = chopped_df.label.value_counts()\n",
    "label_counts.index = label_counts.index.map(get_name)\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Label occurrences in \"chopped\" dataset')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Occurrences')\n",
    "_ = plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "\n",
    "def get_audio(file_path):\n",
    "    audio_file_path = f'{dataset_dir}/{file_path}'\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        return None, None\n",
    "    sample_rate, data_int16 = wavfile.read(audio_file_path)\n",
    "    return sample_rate, (data_int16 / (2**15))  # Convert from int16 to float32\n",
    "\n",
    "def get_clip(row):\n",
    "    sample_rate, track_data = get_audio(row.file_path)\n",
    "    return sample_rate, track_data[row.begin_frame:row.begin_frame + row.num_frames]\n",
    "\n",
    "def preview_clip_row(row):\n",
    "    sample_rate, clip = get_clip(row)\n",
    "    if sample_rate is None:\n",
    "        return None\n",
    "    length = clip.shape[0] / sample_rate\n",
    "    time = np.linspace(0, length, clip.shape[0])\n",
    "    label = row.label\n",
    "    name = labels_df.iloc[label]['name']\n",
    "    session = slim_metadata_df.iloc[row.slim_id]\n",
    "    kit_name = session.kit_name\n",
    "\n",
    "    plt.plot(time, clip, label=f'{name} ({kit_name})')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlim([0, length])\n",
    "    plt.show()\n",
    "\n",
    "    return Audio(clip, rate=sample_rate)\n",
    "\n",
    "def preview_track(audio_filename):\n",
    "    sample_rate, clip = get_audio(audio_filename)\n",
    "    if sample_rate is None:\n",
    "        return None\n",
    "    length = clip.shape[0] / sample_rate\n",
    "    time = np.linspace(0, length, clip.shape[0])\n",
    "\n",
    "    plt.plot(time, clip, label=f'{audio_filename}')\n",
    "\n",
    "    # Get the default y-axis limits (slightly larger than min and max values, which I want to keep).\n",
    "    current_axes = plt.gca()\n",
    "    y_min, y_max = current_axes.get_ylim()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlim([0, length])\n",
    "    plt.ylim([y_min, y_max]) # Need to reset ylim after adding rects to prevent further autoscaling above/below the rects.\n",
    "    plt.show()\n",
    "\n",
    "    return Audio(clip, rate=sample_rate)\n",
    "\n",
    "num_rows = lambda label: len(chopped_df[chopped_df.label == label])\n",
    "get_row = lambda label, index: chopped_df[chopped_df.label == label].iloc[index]\n",
    "preview_clip = lambda label, index: preview_clip_row(get_row(label, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_row(label=2, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the range of index based on the current selected label.\n",
    "def update_index_range(*args):\n",
    "    index_slider.max = num_rows(label_slider.value) - 1\n",
    "\n",
    "def display_preview_clip(label, index):\n",
    "    with preview_output:\n",
    "        preview_output.clear_output()\n",
    "        display(preview_clip(label, index))\n",
    "\n",
    "label_slider = widgets.IntSlider(min=0, max=len(labels_df) - 1, step=1, value=0, description='Label')\n",
    "label_slider.observe(update_index_range, 'value')\n",
    "index_slider = widgets.IntSlider(min=0, max=num_rows(0) - 1, step=1, value=0, description='Index')\n",
    "preview_output = widgets.Output()\n",
    "\n",
    "widgets.interactive(display_preview_clip, label=label_slider, index=index_slider)\n",
    "display(widgets.HBox([label_slider, index_slider]), preview_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_with_end_padding(row, padding_ms=250):\n",
    "    sample_rate, clip = get_clip(row)\n",
    "    return np.concatenate([clip, np.zeros(int(padding_ms * sample_rate / 1000))])\n",
    "\n",
    "def create_supercut(label, num_records=100):\n",
    "    print(f'Creating a random supercut of {num_records} records for label {label} ({get_name(label)})...')\n",
    "    records = chopped_df[chopped_df.label == label].sample(num_records)\n",
    "    supercut = np.concatenate([clip_with_end_padding(row) for _, row in records.iterrows()])\n",
    "    return Audio(supercut, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_supercut(label, num_records):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        description_label.value = get_name(label)\n",
    "        display(create_supercut(label, num_records))\n",
    "\n",
    "label_input = widgets.IntSlider(min=0, max=len(labels_df) - 1, step=1, value=0, description='Label')\n",
    "num_records_input = widgets.IntSlider(min=1, max=100, step=1, value=50, description='Num clips')\n",
    "description_label = widgets.Label(value=get_name(label_input.value))\n",
    "slider_with_description = widgets.HBox([label_input, description_label, num_records_input])\n",
    "output_area = widgets.Output()\n",
    "\n",
    "widgets.interactive(display_supercut, label=label_input, num_records=num_records_input)\n",
    "display(slider_with_description, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_matches(query, max_results=10):\n",
    "    return slim_metadata_df[slim_metadata_df.audio_filename.str.lower().str.contains(query.lower())].audio_filename.unique()[:max_results]\n",
    "\n",
    "def on_text_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        matches = find_closest_matches(change['new'])\n",
    "        dropdown.options = matches\n",
    "\n",
    "text_input = widgets.Text(description='Audio file:')\n",
    "dropdown = widgets.Dropdown(description='Matches:')\n",
    "text_input.observe(on_text_change)\n",
    "\n",
    "def display_preview_track(audio_filename):\n",
    "    with preview_output:\n",
    "        preview_output.clear_output()\n",
    "        display(preview_track(audio_filename))\n",
    "\n",
    "preview_output = widgets.Output()\n",
    "\n",
    "widgets.interactive(display_preview_track, audio_filename=dropdown)\n",
    "display(widgets.VBox([text_input, dropdown]), preview_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
