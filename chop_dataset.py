# This script outputs a CSV at `OUT_CSV_PATH` for training over a slimmed version of the Expanded Groove MIDI Dataset.
# (See `create_slim_metadata.py` for details on the slimmed dataset.)
# It assumes the dataset has been downloaded and extracted to `dataset/e-gmd-v1.0.0`,
# and that the metadata file `dataset/e-gmd-v1.0.0-slim.csv`, generated by the `create_slim_metadata.py` script, is present.
#
# It consists of a row per "drum hit", which is composed of one or more simultaneously sounding drum instruments, and it has the following columns:
# - `file_path`: The path to the audio file in the E-GMD dataset.
# - `begin_frame`: The frame (sample index) of the beginning of the hit.
# - `num_frames`: The length, in frames, of the hit.
# - `label`: A (single) drum instrument label, corresponding to the `id` column in the
#             `dataset/label_mapping.csv` file generated by the `create_label_mapping.py` script.
# - `slim_id`: The session ID (index in `e-gmd-v1.0.0-slim.csv`) in which this hit was found, for access to any other metadata.

from mido import MidiFile
import pandas as pd

DATASET_DIR = 'dataset/e-gmd-v1.0.0'
SLIM_METADATA_PATH = 'dataset/e-gmd-v1.0.0-slim.csv'
LABEL_MAPPING_PATH = 'dataset/label_mapping.csv'

CHOPPED_OUT_CSV_PATH = 'dataset/chopped.csv'
# Output dataset column names
FILE_PATH_COL = 'file_path'
BEGIN_FRAME_COL = 'begin_frame'
NUM_FRAMES_COL = 'num_frames'
LABEL_COL = 'label'
SLIM_ID_COL = 'slim_id'

# E-GMD metadata column names
DRUMMER = 'drummer'
SESSION = 'session'
ID = 'id'
STYLE = 'style'
BPM = 'bpm'
BEAT_TYPE = 'beat_type' # 'beat' or 'fill'
TIME_SIGNATURE = 'time_signature'
DURATION = 'duration'
SPLIT = 'split' # 'train', 'test', or 'validation'
MIDI_FILENAME = 'midi_filename'
AUDIO_FILENAME = 'audio_filename'
KIT_NAME = 'kit_name'

SAMPLE_RATE = 44_100

ms_to_frames = lambda ms: round(ms * SAMPLE_RATE / 1000)

# Minimum length of a drum hit.
MIN_HIT_FRAMES = ms_to_frames(200)
# All hits occurring within this many frames of each other are considered part of the same drum hit and are ignored.
# This is a heuristic to try and find monophonic drum hits.
MIN_HIT_GAP_FRAMES = ms_to_frames(100)
# Length of silence to trim before the beginning of the following drum hit when determining the length of the active drum hit.
# This helps prevent small sections of the neighboring drum hits from being included, due to imprecision in the MIDI timing.
TRIM_HIT_FRAMES = ms_to_frames(10)

# Appends a row per "drum hit" to the provided `dataset`.
# A drum hit candidate starts with a non-zero velocity note-on event, and continues until the next non-zero velocity note-on event.
# Only candidates with a length of at least 100ms are considered.
# A "drum hit" is a segment of audio at least 100ms long containing exactly one drum instrument.
# `dataset` is expected to have the dataset columns (described above) as keys, with list values.
# `slim_id` is session's index in the slimmed metadata CSV file.
def append_records(dataset, label_mapping_df, session_metadata, slim_id):
    audio_file_path = session_metadata[AUDIO_FILENAME]
    midi_file_path = f'{DATASET_DIR}/{session_metadata[MIDI_FILENAME]}'

    midi_file = MidiFile(midi_file_path)
    assert(len(midi_file.tracks) == 2) # First track is metadata, then one track of drum notes.

    # Add the hit it to `dataset` if valid.
    # A hit is a dict with keys BEGIN_FRAME_COL and LABEL_COL.
    def check_append_hit(hit, end_frame):
        if hit is None:
            return
        trimmed_end_frame = int(end_frame - TRIM_HIT_FRAMES)
        num_frames = int(trimmed_end_frame - hit[BEGIN_FRAME_COL])
        if num_frames < MIN_HIT_FRAMES:
            return
        dataset[FILE_PATH_COL].append(audio_file_path),
        dataset[BEGIN_FRAME_COL].append(hit[BEGIN_FRAME_COL]),
        dataset[NUM_FRAMES_COL].append(num_frames),
        dataset[LABEL_COL].append(hit[LABEL_COL])
        dataset[SLIM_ID_COL].append(slim_id)

    def get_label(note):
        match = label_mapping_df[label_mapping_df.note == note]
        return None if len(match) == 0 else match.iloc[0].id

    # [Roland TD-17 MIDI implementation spec](https://static.roland.com/assets/media/pdf/TD-17_MIDI_Imple_eng01_W.pdf)
    # It doesn't say this in the spec, but it seems that note off events are sent as note on events with velocity 0.

    # A note is considered active if it occurred within the last `MIN_HIT_GAP_FRAMES` frames.
    # This is a dict from note number to its note on frame.
    active_notes = dict()
    active_hit = None
    total_time_sec = 0
    for msg in midi_file:
        total_time_sec += msg.time # Delta time
        if msg.type == 'note_on' and msg.velocity > 0:
            frame = round(total_time_sec * SAMPLE_RATE)
            # Expire any active notes that occurred more than `MIN_HIT_GAP_FRAMES` frames ago, and add the new note.
            expired_notes = [note for note, note_frame in active_notes.items() if frame - note_frame > MIN_HIT_GAP_FRAMES]
            for note in expired_notes:
                del active_notes[note]
            any_active_notes = len(active_notes) > 0
            active_notes[msg.note] = frame
            if any_active_notes:
                active_hit = None
                continue
            check_append_hit(active_hit, frame)
            active_hit = None
            label = get_label(msg.note)
            if label is not None: # Ignore notes that aren't in the label mapping.
                active_hit = {BEGIN_FRAME_COL: frame, LABEL_COL: label}

    # End the final hit, if present.
    check_append_hit(active_hit, session_metadata[DURATION] * SAMPLE_RATE - 1)

if __name__ == '__main__':
    metadata_df = pd.read_csv(SLIM_METADATA_PATH)
    label_mapping_df = pd.read_csv(LABEL_MAPPING_PATH)
    dataset = {FILE_PATH_COL: [], BEGIN_FRAME_COL: [], NUM_FRAMES_COL: [], LABEL_COL: [], SLIM_ID_COL: []}
    for i, session_metadata in metadata_df.iterrows():
        print(f'Processing {session_metadata[MIDI_FILENAME]}...')
        append_records(dataset, label_mapping_df, session_metadata, i)
    
    print('')
    print(f'Saving chopped dataset of {len(dataset[FILE_PATH_COL])} rows to {CHOPPED_OUT_CSV_PATH}.')
    df = pd.DataFrame(dataset)
    df.to_csv(CHOPPED_OUT_CSV_PATH, index=False)
